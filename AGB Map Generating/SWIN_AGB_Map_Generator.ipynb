{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf  # For tf.data and preprocessing only.\n",
    "# import tensorflow_addons as tfa\n",
    "import keras\n",
    "from keras import layers\n",
    "# from keras import ops\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score, recall_score, precision_score, cohen_kappa_score\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler, ReduceLROnPlateau, EarlyStopping\n",
    "from skimage.io import imread, imshow\n",
    "from skimage.transform import resize\n",
    "from skimage import io  # for reading TIFF images\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import h5py\n",
    "import numpy as np\n",
    "from skimage import io  # for reading TIFF images\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from osgeo import gdal\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 1\n",
    "input_shape = (90, 90, 15)\n",
    "\n",
    "patch_size = (5, 5)  # 2-by-2 sized patches\n",
    "dropout_rate = 0.03  # Dropout rate\n",
    "num_heads = 32  # Attention heads\n",
    "embed_dim = 64  # Embedding dimension\n",
    "num_mlp = 512  # MLP layer size\n",
    "# Convert embedded patches to query, key, and values with a learnable additive\n",
    "# value\n",
    "qkv_bias = True\n",
    "window_size = 2  # Size of attention window\n",
    "shift_size = 1  # Size of shifting window\n",
    "image_dimension = 72  # Initial image size\n",
    "\n",
    "num_patch_x = input_shape[0] // patch_size[0]\n",
    "num_patch_y = input_shape[1] // patch_size[1]\n",
    "\n",
    "learning_rate = 1e-4\n",
    "batch_size = 256\n",
    "num_epochs = 80\n",
    "weight_decay = 0.00001\n",
    "label_smoothing = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the desired number of patches\n",
    "desired_num_patches = 500\n",
    "\n",
    "# Your data directories\n",
    "optic_images_dir = r'F:\\Sent2'\n",
    "sar_images_dir = r'F:\\Sent1'\n",
    "agb_maps_dir = r'F:\\AGB'\n",
    "\n",
    "# Get the list of file names in the directories without sorting\n",
    "agb_map_files = os.listdir(agb_maps_dir)\n",
    "optic_image_files = os.listdir(optic_images_dir)\n",
    "sar_image_files = os.listdir(sar_images_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Patches: 100%|█████████▉| 499/500 [00:12<00:00, 40.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_index: [361, 73, 374, 155, 104, 394, 377, 124, 68, 450, 9, 194, 406, 84, 371, 388, 495, 30, 316, 408, 490, 491, 280, 356, 76, 461, 497, 211, 101, 334, 475, 336, 440, 173, 2, 333, 409, 70, 209, 63, 384, 93, 485, 185, 33, 77, 0, 11, 415, 22, 72, 182, 131, 410, 193, 55, 148, 18, 204, 78, 494, 262, 323, 483, 79, 39, 451, 46, 238, 391, 352, 341, 277, 290, 317, 304, 268, 69, 455, 465, 154, 82, 477, 172, 321, 90, 180, 414, 312, 278, 381, 472, 362, 324, 431, 347, 86, 75, 438, 15]\n",
      "train_index: [3, 18, 202, 250, 274, 63, 248, 301, 108, 90, 233, 335, 118, 220, 180, 314, 373, 380, 239, 75, 247, 110, 16, 66, 153, 7, 19, 137, 355, 349, 131, 387, 292, 386, 297, 60, 79, 285, 305, 281, 157, 109, 17, 347, 24, 175, 351, 332, 167, 245, 311, 145, 258, 177, 119, 194, 229, 265, 218, 307, 36, 139, 291, 155, 196, 284, 59, 111, 165, 363, 6, 333, 275, 150, 10, 192, 103, 81, 316, 398, 375, 208, 286, 168, 353, 89, 377, 163, 147, 228, 92, 69, 123, 96, 143, 326, 97, 238, 68, 23, 37, 144, 122, 182, 67, 198, 324, 219, 317, 195, 125, 146, 371, 86, 336, 340, 337, 354, 334, 302, 183, 203, 283, 211, 282, 129, 38, 11, 325, 185, 222, 112, 179, 327, 299, 117, 368, 277, 384, 287, 362, 164, 253, 136, 154, 199, 197, 392, 2, 204, 115, 234, 237, 26, 120, 357, 224, 360, 378, 350, 379, 127, 74, 322, 29, 83, 389, 107, 249, 296, 133, 244, 184, 290, 44, 367, 65, 85, 242, 186, 159, 12, 35, 28, 170, 142, 318, 272, 221, 95, 51, 240, 298, 304, 178, 41, 310, 206, 254, 331, 4, 256, 358, 100, 226, 341, 213, 171, 98, 215, 61, 47, 32, 267, 200, 356, 27, 312, 230, 260, 288, 162, 370, 138, 62, 135, 128, 8, 64, 300, 14, 156, 40, 366, 323, 216, 279, 346, 236, 207, 212, 295, 364, 251, 390, 365, 303, 269, 201, 161, 43, 217, 190, 309, 259, 105, 53, 1, 49, 80, 205, 34, 263, 91, 339, 52, 345, 264, 241, 13, 315, 88, 273, 166, 328, 393, 134, 306, 383, 319, 243, 54, 50, 174, 189, 397, 187, 169, 58, 48, 344, 235, 252, 21, 313, 160, 276, 191, 293, 343, 257, 308, 149, 130, 151, 359, 99, 372, 87, 330, 214, 121, 399, 20, 188, 71, 106, 270, 348, 102]\n",
      "val_index: [361, 73, 374, 155, 104, 394, 377, 124, 68, 450, 9, 194, 406, 84, 371, 388, 495, 30, 316, 408, 490, 491, 280, 356, 76, 461, 497, 211, 101, 334, 475, 336, 440, 173, 2, 333, 409, 70, 209, 63, 384, 93, 485, 185, 33, 77, 0, 11, 415, 22, 72, 182, 131, 410, 193, 55, 148, 18, 204, 78, 494, 262, 323, 483, 79, 39, 451, 46, 238, 391, 352, 341, 277, 290, 317, 304, 268, 69, 455, 465, 154, 82, 477, 172, 321, 90, 180, 414, 312, 278, 381, 472, 362, 324, 431, 347, 86, 75, 438, 15]\n",
      "x_train: (320, 90, 90, 15)\n",
      "y_train: (320, 1)\n",
      "x_val shape: (80, 90, 90, 15)\n",
      "y_val shape: (80, 1)\n",
      "x_test shape: (100, 90, 90, 15)\n",
      "y_test shape: (100, 1)\n"
     ]
    }
   ],
   "source": [
    "# Initialize empty lists to store data\n",
    "x_train_list = []\n",
    "y_train_list = []\n",
    "agb_values_list = []\n",
    "\n",
    "# Loop through each patch with tqdm for progress bars\n",
    "# Loop through each patch with tqdm for progress bars\n",
    "for i in tqdm(range(desired_num_patches), desc='Loading Patches'):\n",
    "    # Get file names for the current index\n",
    "    optic_file = optic_image_files[i]\n",
    "    sar_file = sar_image_files[i]\n",
    "    agb_file = agb_map_files[i]\n",
    "    # Load images\n",
    "    optic_image = io.imread(os.path.join(optic_images_dir, optic_file))\n",
    "    sar_image = io.imread(os.path.join(sar_images_dir, sar_file))\n",
    "    agb_map = io.imread(os.path.join(agb_maps_dir, agb_file))\n",
    "\n",
    "    # Save AGB values as a NumPy array\n",
    "    agb_values_list.append(agb_map.flatten())\n",
    "\n",
    "    # Ensure optic image has 13 bands\n",
    "    if optic_image.shape[-1] == 13:\n",
    "        x_train = optic_image\n",
    "    else:\n",
    "        raise ValueError(\"Optic image should have 13 bands. Found: {}\".format(optic_image.shape[-1]))\n",
    "\n",
    "    # Check if SAR image has 2 bands\n",
    "    if len(sar_image.shape) == 2:\n",
    "        # Expand dimensions to make it 3D\n",
    "        sar_image = np.stack([sar_image, sar_image], axis=-1)\n",
    "\n",
    "    # Normalize the data using min-max scaling\n",
    "    optic_image_normalized = (optic_image - np.min(optic_image)) / (np.max(optic_image) - np.min(optic_image))\n",
    "    sar_image_normalized = (sar_image - np.min(sar_image)) / (np.max(sar_image) - np.min(sar_image))\n",
    "\n",
    "    # Concatenate optic and SAR images\n",
    "    x_train = np.concatenate([optic_image_normalized, sar_image_normalized], axis=-1)\n",
    "\n",
    "    x_train_list.append(x_train)\n",
    "\n",
    "    # Break the loop when desired_num_patches is reached\n",
    "    if len(x_train_list) >= desired_num_patches:\n",
    "        break\n",
    "\n",
    "# Combine all patches into single arrays\n",
    "x_data = np.stack(x_train_list)\n",
    "agb_values = np.concatenate(agb_values_list)\n",
    "\n",
    "# Normalize AGB values using min-max scaling\n",
    "min_agb = np.min(agb_values)\n",
    "max_agb = np.max(agb_values)\n",
    "agb_scaled = (agb_values - min_agb) / (max_agb - min_agb)\n",
    "\n",
    "# Reshape scaled AGB values for compatibility with the model\n",
    "agb_normalized = np.expand_dims(agb_scaled, axis=-1)\n",
    "\n",
    "# Split the data into training, validation, and test sets\n",
    "\n",
    "x_train1, x_test, y_train1, y_test, _ , test_index = train_test_split(x_data, agb_normalized,range(len(x_data)), test_size=0.20, random_state=42)\n",
    "x_train, x_val, y_train, y_val, train_index, val_index = train_test_split(x_train1, y_train1, range(len(x_train1)), test_size=0.20, random_state=42)\n",
    "\n",
    "print('test_index:', test_index)\n",
    "print('train_index:', train_index)\n",
    "print('val_index:', test_index)\n",
    "print('x_train:', x_train.shape)\n",
    "print('y_train:', y_train.shape)\n",
    "print('x_val shape:', x_val.shape)\n",
    "print('y_val shape:', y_val.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "print('y_test shape:', y_test.shape)\n",
    "input_shape = x_train.shape[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def window_partition(x, window_size):\n",
    "    _, height, width, channels = x.shape\n",
    "    patch_num_y = height // window_size\n",
    "    patch_num_x = width // window_size\n",
    "    x = tf.reshape(\n",
    "        x,\n",
    "        (\n",
    "            -1,\n",
    "            patch_num_y,\n",
    "            window_size,\n",
    "            patch_num_x,\n",
    "            window_size,\n",
    "            channels,\n",
    "        ),\n",
    "    )\n",
    "    x = tf.transpose(x, (0, 1, 3, 2, 4, 5))\n",
    "    windows = tf.reshape(x, (-1, window_size, window_size, channels))\n",
    "    return windows\n",
    "\n",
    "\n",
    "def window_reverse(windows, window_size, height, width, channels):\n",
    "    patch_num_y = height // window_size\n",
    "    patch_num_x = width // window_size\n",
    "    x = tf.reshape(\n",
    "        windows,\n",
    "        (\n",
    "            -1,\n",
    "            patch_num_y,\n",
    "            patch_num_x,\n",
    "            window_size,\n",
    "            window_size,\n",
    "            channels,\n",
    "        ),\n",
    "    )\n",
    "    x = tf.transpose(x, (0, 1, 3, 2, 4, 5))\n",
    "    x = tf.reshape(x, (-1, height, width, channels))\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WindowAttention(layers.Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        window_size,\n",
    "        num_heads,\n",
    "        qkv_bias=True,\n",
    "        dropout_rate=0.0,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.dim = dim\n",
    "        self.window_size = window_size\n",
    "        self.num_heads = num_heads\n",
    "        self.scale = (dim // num_heads) ** -0.5\n",
    "        self.qkv = layers.Dense(dim * 3, use_bias=qkv_bias)\n",
    "        self.dropout = layers.Dropout(dropout_rate)\n",
    "        self.proj = layers.Dense(dim)\n",
    "\n",
    "        num_window_elements = (2 * self.window_size[0] - 1) * (\n",
    "            2 * self.window_size[1] - 1\n",
    "        )\n",
    "        self.relative_position_bias_table = self.add_weight(\n",
    "            shape=(num_window_elements, self.num_heads),\n",
    "            initializer=keras.initializers.Zeros(),\n",
    "            trainable=True,\n",
    "        )\n",
    "        coords_h = np.arange(self.window_size[0])\n",
    "        coords_w = np.arange(self.window_size[1])\n",
    "        coords_matrix = np.meshgrid(coords_h, coords_w, indexing=\"ij\")\n",
    "        coords = np.stack(coords_matrix)\n",
    "        coords_flatten = coords.reshape(2, -1)\n",
    "        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]\n",
    "        relative_coords = relative_coords.transpose([1, 2, 0])\n",
    "        relative_coords[:, :, 0] += self.window_size[0] - 1\n",
    "        relative_coords[:, :, 1] += self.window_size[1] - 1\n",
    "        relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1\n",
    "        relative_position_index = relative_coords.sum(-1)\n",
    "\n",
    "        # self.relative_position_index = keras.Variable(\n",
    "         #   initializer=relative_position_index,\n",
    "        #    shape=relative_position_index.shape,\n",
    "         #   dtype=\"int\",\n",
    "        #    trainable=False,\n",
    "        #)\n",
    "\n",
    "\n",
    "# Define self.relative_position_index using tf.Variable\n",
    "        self.relative_position_index = tf.Variable(\n",
    "                initial_value=relative_position_index,\n",
    "                trainable=False,\n",
    "                dtype=tf.int32  # Set the data type to int32\n",
    "        )\n",
    "\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        _, size, channels = x.shape\n",
    "        head_dim = channels // self.num_heads\n",
    "        x_qkv = self.qkv(x)\n",
    "        x_qkv = tf.reshape(x_qkv, (-1, size, 3, self.num_heads, head_dim))\n",
    "        x_qkv = tf.transpose(x_qkv, (2, 0, 3, 1, 4))\n",
    "        q, k, v = x_qkv[0], x_qkv[1], x_qkv[2]\n",
    "        q = q * self.scale\n",
    "        k = tf.transpose(k, (0, 1, 3, 2))\n",
    "        attn = q @ k\n",
    "\n",
    "        num_window_elements = self.window_size[0] * self.window_size[1]\n",
    "        relative_position_index_flat = tf.reshape(self.relative_position_index, (-1,))\n",
    "        relative_position_bias = tf.gather(\n",
    "            self.relative_position_bias_table,\n",
    "            relative_position_index_flat,\n",
    "            axis=0,\n",
    "        )\n",
    "        relative_position_bias = tf.reshape(\n",
    "            relative_position_bias,\n",
    "            (num_window_elements, num_window_elements, -1),\n",
    "        )\n",
    "        relative_position_bias = tf.transpose(relative_position_bias, (2, 0, 1))\n",
    "        attn = attn + tf.expand_dims(relative_position_bias, axis=0)\n",
    "\n",
    "        if mask is not None:\n",
    "            nW = mask.shape[0]\n",
    "            mask_float = tf.cast(\n",
    "                tf.expand_dims(tf.expand_dims(mask, axis=1), axis=0),\n",
    "                \"float32\",\n",
    "            )\n",
    "            attn = tf.reshape(attn, (-1, nW, self.num_heads, size, size)) + mask_float\n",
    "            attn = tf.reshape(attn, (-1, self.num_heads, size, size))\n",
    "            attn = keras.activations.softmax(attn, axis=-1)\n",
    "        else:\n",
    "            attn = keras.activations.softmax(attn, axis=-1)\n",
    "        attn = self.dropout(attn)\n",
    "\n",
    "        x_qkv = attn @ v\n",
    "        x_qkv = tf.transpose(x_qkv, (0, 2, 1, 3))\n",
    "        x_qkv = tf.reshape(x_qkv, (-1, size, channels))\n",
    "        x_qkv = self.proj(x_qkv)\n",
    "        x_qkv = self.dropout(x_qkv)\n",
    "        return x_qkv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SwinTransformer(layers.Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        num_patch,\n",
    "        num_heads,\n",
    "        window_size=5,\n",
    "        shift_size=1,\n",
    "        num_mlp=1024,\n",
    "        qkv_bias= True,\n",
    "        dropout_rate=0.0,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.dim = dim  # number of input dimensions\n",
    "        self.num_patch = num_patch  # number of embedded patches\n",
    "        self.num_heads = num_heads  # number of attention heads\n",
    "        self.window_size = window_size  # size of window\n",
    "        self.shift_size = shift_size  # size of window shift\n",
    "        self.num_mlp = num_mlp  # number of MLP nodes\n",
    "\n",
    "        self.norm1 = layers.LayerNormalization(epsilon=1e-5)\n",
    "        self.attn = WindowAttention(\n",
    "            dim,\n",
    "            window_size=(self.window_size, self.window_size),\n",
    "            num_heads=num_heads,\n",
    "            qkv_bias=qkv_bias,\n",
    "            dropout_rate=dropout_rate,\n",
    "        )\n",
    "        self.drop_path = layers.Dropout(dropout_rate)\n",
    "        self.norm2 = layers.LayerNormalization(epsilon=1e-5)\n",
    "\n",
    "        self.mlp = keras.Sequential(\n",
    "            [\n",
    "                layers.Dense(num_mlp),\n",
    "                layers.Activation(keras.activations.gelu),\n",
    "                layers.Dropout(dropout_rate),\n",
    "                layers.Dense(dim),\n",
    "                layers.Dropout(dropout_rate),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        if min(self.num_patch) < self.window_size:\n",
    "            self.shift_size = 0\n",
    "            self.window_size = min(self.num_patch)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        if self.shift_size == 0:\n",
    "            self.attn_mask = None\n",
    "        else:\n",
    "            height, width = self.num_patch\n",
    "            h_slices = (\n",
    "                slice(0, -self.window_size),\n",
    "                slice(-self.window_size, -self.shift_size),\n",
    "                slice(-self.shift_size, None),\n",
    "            )\n",
    "            w_slices = (\n",
    "                slice(0, -self.window_size),\n",
    "                slice(-self.window_size, -self.shift_size),\n",
    "                slice(-self.shift_size, None),\n",
    "            )\n",
    "            mask_array = np.zeros((1, height, width, 1))\n",
    "            count = 0\n",
    "            for h in h_slices:\n",
    "                for w in w_slices:\n",
    "                    mask_array[:, h, w, :] = count\n",
    "                    count += 1\n",
    "            mask_array = tf.convert_to_tensor(mask_array)\n",
    "\n",
    "            # mask array to windows\n",
    "            mask_windows = window_partition(mask_array, self.window_size)\n",
    "            mask_windows = tf.reshape(\n",
    "                mask_windows, [-1, self.window_size * self.window_size]\n",
    "            )\n",
    "            attn_mask = tf.expand_dims(mask_windows, axis=1) - tf.expand_dims(\n",
    "                mask_windows, axis=2\n",
    "            )\n",
    "            attn_mask = tf.where(attn_mask != 0, -100.0, attn_mask)\n",
    "            attn_mask = tf.where(attn_mask == 0, 0.0, attn_mask)\n",
    "            self.attn_mask = tf.Variable(\n",
    "                initial_value=attn_mask,\n",
    "\n",
    "                dtype=attn_mask.dtype,\n",
    "                trainable=False,\n",
    "            )\n",
    "\n",
    "\n",
    "    def call(self, x, training=False):\n",
    "        height, width = self.num_patch\n",
    "        _, num_patches_before, channels = x.shape\n",
    "        x_skip = x\n",
    "        x = self.norm1(x)\n",
    "        x = tf.reshape(x, (-1, height, width, channels))\n",
    "        if self.shift_size > 0:\n",
    "            shifted_x = tf.roll(\n",
    "                x, shift=[-self.shift_size, -self.shift_size], axis=[1, 2]\n",
    "            )\n",
    "        else:\n",
    "            shifted_x = x\n",
    "\n",
    "        x_windows = window_partition(shifted_x, self.window_size)\n",
    "        x_windows = tf.reshape(\n",
    "            x_windows, (-1, self.window_size * self.window_size, channels)\n",
    "        )\n",
    "        attn_windows = self.attn(x_windows, mask=self.attn_mask)\n",
    "\n",
    "        attn_windows = tf.reshape(\n",
    "            attn_windows,\n",
    "            (-1, self.window_size, self.window_size, channels),\n",
    "        )\n",
    "        shifted_x = window_reverse(\n",
    "            attn_windows, self.window_size, height, width, channels\n",
    "        )\n",
    "        if self.shift_size > 0:\n",
    "            x = tf.roll(\n",
    "                shifted_x, shift=[self.shift_size, self.shift_size], axis=[1, 2]\n",
    "            )\n",
    "        else:\n",
    "            x = shifted_x\n",
    "\n",
    "        x = tf.reshape(x, (-1, height * width, channels))\n",
    "        x = self.drop_path(x, training=training)\n",
    "        x = x_skip + x\n",
    "        x_skip = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.mlp(x)\n",
    "        x = self.drop_path(x)\n",
    "        x = x_skip + x\n",
    "        return x\n",
    "\n",
    "    def get_config(self):\n",
    "      config = super().get_config()\n",
    "      config.update({\n",
    "        \"dim\": self.dim,\n",
    "        \"num_patch\": self.num_patch,\n",
    "        \"num_heads\": self.num_heads,\n",
    "        \"window_size\": self.window_size,\n",
    "        \"shift_size\": self.shift_size,\n",
    "        \"num_mlp\": self.num_mlp,\n",
    "\n",
    "      })\n",
    "      return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Using tf ops since it is only used in tf.data.\n",
    "def patch_extract(images):\n",
    "    batch_size = tf.shape(images)[0]\n",
    "    patches = tf.image.extract_patches(\n",
    "        images=images,\n",
    "        sizes=(1, patch_size[0], patch_size[1], 1),\n",
    "        strides=(1, patch_size[0], patch_size[1], 1),\n",
    "        rates=(1, 1, 1, 1),\n",
    "        padding=\"VALID\",\n",
    "    )\n",
    "    patch_dim = patches.shape[-1]\n",
    "    patch_num = patches.shape[1]\n",
    "    return tf.reshape(patches, (batch_size, patch_num * patch_num, patch_dim))\n",
    "\n",
    "\n",
    "class PatchEmbedding(layers.Layer):\n",
    "    def __init__(self, num_patch, embed_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.num_patch = num_patch\n",
    "        self.embed_dim = embed_dim\n",
    "        self.proj = layers.Dense(embed_dim)\n",
    "        self.pos_embed = layers.Embedding(input_dim=num_patch, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, patch):\n",
    "        pos = tf.range(start=0, limit=self.num_patch)\n",
    "        return self.proj(patch) + self.pos_embed(pos)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            'num_patch': self.num_patch,\n",
    "            'embed_dim': self.embed_dim,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "\n",
    "class PatchMerging(keras.layers.Layer):\n",
    "    def __init__(self, num_patch, embed_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.num_patch = num_patch\n",
    "        self.embed_dim = embed_dim\n",
    "        self.linear_trans = layers.Dense(2 * embed_dim, use_bias=False)\n",
    "\n",
    "    def call(self, x):\n",
    "        height, width = self.num_patch\n",
    "        _, _, C = x.shape\n",
    "        x = tf.reshape(x, (-1, height, width, C))\n",
    "        x0 = x[:, 0::2, 0::2, :]\n",
    "        x1 = x[:, 1::2, 0::2, :]\n",
    "        x2 = x[:, 0::2, 1::2, :]\n",
    "        x3 = x[:, 1::2, 1::2, :]\n",
    "        x = tf.concat((x0, x1, x2, x3), axis=-1)\n",
    "        x = tf.reshape(x, (-1, (height // 2) * (width // 2), 4 * C))\n",
    "        return self.linear_trans(x)\n",
    "    def get_config(self):\n",
    "          config = super().get_config()\n",
    "          config.update({\n",
    "            'num_patch': self.num_patch,\n",
    "            'embed_dim': self.embed_dim,\n",
    "\n",
    "          })\n",
    "          return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28412\n",
      "261112\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(min_agb)\n",
    "print(max_agb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "# Load the saved model\n",
    "loaded_model = keras.models.load_model('best_model.keras', custom_objects={'SwinTransformer': SwinTransformer,\n",
    "                                                                            'WindowAttention': WindowAttention,\n",
    "                                                                            'PatchEmbedding': PatchEmbedding,\n",
    "                                                                            'PatchMerging': PatchMerging})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment(x):\n",
    "    x = tf.image.random_crop(x, size=(image_dimension, image_dimension, 15))\n",
    "    x = tf.image.random_flip_left_right(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "dataset_predict = (\n",
    "    tf.data.Dataset.from_tensor_slices((x_data, agb_normalized))\n",
    "    .batch(batch_size=batch_size)\n",
    "    .map(lambda x, y: (patch_extract(x), y))\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 4s 4s/step\n"
     ]
    }
   ],
   "source": [
    "y_predX = loaded_model.predict(dataset_predict)\n",
    "y_pred = y_predX * (max_agb - min_agb) + min_agb  # Denormalize predictions\n",
    "y_denormalized = agb_normalized * ((max_agb - min_agb) + min_agb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from osgeo import gdal\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "for i in range(3480):\n",
    "\n",
    "  ff = agb_map_files [i].split(\".\")\n",
    "  #print(ff[0])\n",
    "  input_file = gdal.Open(os.path.join(agb_maps_dir, f\"{ff[0]}.tif\"))\n",
    "  #print(input_file.RasterXSize)\n",
    "  # Create the output GeoTIFF file\n",
    "  output_file = gdal.GetDriverByName('GTiff').Create(r\"F:\\SWIN\" \n",
    "                                                     + f\"{ff[0]}\" + '.tif', input_file.RasterXSize, \n",
    "                                                     input_file.RasterYSize, 1, gdal.GDT_Float32)\n",
    "\n",
    "  # Set the spatial reference and geotransform of the output file\n",
    "  output_file.SetProjection(input_file.GetProjection())\n",
    "  output_file.SetGeoTransform(input_file.GetGeoTransform())\n",
    "\n",
    "  # Get the output band of the output file\n",
    "  output_band = output_file.GetRasterBand(1)\n",
    "  output_band.WriteArray(y_pred[i].reshape(input_file.RasterXSize,input_file.RasterYSize))\n",
    "\n",
    "  # Compute the output band's statistics\n",
    "  output_band.ComputeStatistics(False)\n",
    "\n",
    "  # Close the input and output files\n",
    "  input_file = None\n",
    "  output_file = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "def copy_images(source_folder, destination_folder, image_list):\n",
    "    if not os.path.exists(destination_folder):\n",
    "        os.makedirs(destination_folder)\n",
    "\n",
    "    for filename in image_list:\n",
    "      filename = str(int(filename) + 1).zfill(5) + \".tif\"\n",
    "      print(filename)\n",
    "      source_path = os.path.join(source_folder, filename)\n",
    "      if os.path.exists(source_path):\n",
    "            destination_path = os.path.join(destination_folder, filename)\n",
    "            shutil.copyfile(source_path, destination_path)\n",
    "            print(f\"Copied {filename} to {destination_folder}\")\n",
    "      else:\n",
    "            print(f\"File {filename} not found in {source_folder}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define source folder containing TIFF images\n",
    "source_folder = \"F:\\output\"\n",
    "\n",
    "# Define destination folder where images will be copied\n",
    "destination_folder_Test = \"F:\\Test_OutPut\"\n",
    "destination_folder_Train = \"F:\\Train_OutPut\"\n",
    "destination_folder_Val = \"F:\\Val_OutPut\"\n",
    "\n",
    "# List of filenames to be copied\n",
    "#image_list = [\"image1.tif\", \"image2.tif\", \"image3.tif\"]  # Add your filenames here\n",
    "\n",
    "copy_images(source_folder, destination_folder_Test, test_index)\n",
    "copy_images(source_folder, destination_folder_Train, train_index)\n",
    "copy_images(source_folder, destination_folder_Val, val_index)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
