{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\irannejad\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "import skimage\n",
    "from skimage import io  # for reading TIFF images\n",
    "from tqdm import tqdm  # for progress bars\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from osgeo import gdal\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the desired number of patches\n",
    "desired_num_patches = 3480\n",
    "\n",
    "# Your data directories\n",
    "optic_images_dir = r'F:\\Sent2'\n",
    "sar_images_dir = r'F:\\Sent1'\n",
    "agb_maps_dir = r'F:\\AGB'\n",
    "\n",
    "# Get the list of file names in the directories without sorting\n",
    "agb_map_files = os.listdir(agb_maps_dir)\n",
    "optic_image_files = os.listdir(optic_images_dir)\n",
    "sar_image_files = os.listdir(sar_images_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Patches:   0%|          | 0/3480 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Patches: 100%|█████████▉| 3479/3480 [04:24<00:00, 13.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_index: [2457, 1114, 1298, 1442, 1665, 51, 322, 2653, 2164, 676, 1192, 1864, 2815, 2742, 134, 2004, 1623, 3107, 1320, 1842, 2037, 693, 184, 2658, 192, 1904, 3271, 450, 1146, 621, 2832, 538, 2266, 1537, 2801, 26, 999, 1718, 179, 32, 1468, 1490, 921, 2804, 2383, 2932, 1937, 893, 70, 1068, 2248, 691, 443, 2835, 1584, 298, 1429, 1878, 1736, 678, 1786, 670, 969, 183, 978, 1553, 2381, 3442, 257, 3269, 1234, 1258, 2910, 3019, 2543, 1615, 578, 2603, 1666, 3330, 557, 139, 1966, 2473, 1432, 2115, 295, 1738, 1784, 196, 2484, 2967, 1556, 925, 3229, 942, 2739, 3204, 52, 3374, 1037, 3167, 1091, 495, 3124, 170, 1662, 3310, 478, 309, 2836, 402, 76, 2740, 965, 2059, 17, 3255, 144, 3055, 602, 551, 2023, 2943, 3361, 2189, 3315, 1173, 2717, 3394, 1765, 2957, 1034, 1501, 194, 1871, 1228, 1569, 2154, 3148, 1897, 1832, 1746, 1818, 1600, 30, 879, 1626, 2367, 2643, 985, 869, 2551, 203, 501, 2534, 897, 1313, 3206, 1397, 3344, 2891, 2587, 1674, 1374, 33, 1023, 948, 665, 2830, 3111, 1018, 949, 3197, 3358, 1805, 229, 411, 2475, 1151, 2241, 567, 2313, 2775, 289, 432, 2656, 3474, 410, 1798, 414, 3153, 937, 712, 2599, 3264, 1702, 3329, 2149, 1309, 864, 415, 881, 1742, 952, 811, 279, 781, 479, 2044, 2226, 2323, 80, 1041, 2980, 93, 3240, 1395, 1749, 511, 3276, 463, 1253, 2086, 3362, 2029, 1457, 1593, 1222, 358, 2862, 1729, 1721, 3430, 839, 2272, 3458, 1809, 2339, 798, 1052, 1825, 1860, 506, 166, 1123, 2533, 1211, 3220, 3469, 787, 1467, 430, 874, 2123, 238, 315, 2930, 2732, 1200, 752, 174, 1419, 1420, 1406, 3290, 321, 3267, 2394, 102, 299, 239, 2018, 1071, 325, 761, 2428, 2208, 2803, 44, 1336, 964, 2380, 1351, 1161, 457, 291, 3245, 58, 679, 3472, 387, 3423, 759, 912, 1345, 3288, 2090, 2249, 1340, 1691, 314, 816, 461, 2500, 188, 1322, 2202, 1213, 1427, 393, 240, 2552, 3014, 3243, 1105, 2707, 2757, 1370, 2168, 2271, 152, 2443, 2364, 3401, 573, 256, 1061, 1703, 2345, 1756, 544, 3435, 1922, 2828, 178, 1833, 3035, 439, 3096, 1455, 3462, 2602, 2247, 594, 2461, 14, 486, 173, 976, 331, 1580, 1803, 2020, 1080, 1960, 3332, 2654, 929, 1352, 3052, 1497, 2315, 680, 1586, 527, 785, 63, 266, 1399, 564, 157, 3121, 2093, 2954, 1391, 254, 2369, 1618, 2436, 2407, 124, 1261, 1745, 263, 2308, 3291, 3221, 3173, 1187, 2316, 3287, 2756, 438, 88, 2441, 840, 1624, 2124, 1642, 1206, 1532, 2418, 3000, 3140, 1444, 120, 3359, 2516, 2570, 2764, 2328, 1767, 642, 1074, 283, 1883, 274, 721, 109, 1113, 940, 214, 2456, 3048, 1292, 1232, 3304, 1032, 368, 1812, 1517, 862, 2259, 2513, 1116, 1392, 1582, 2341, 2728, 45, 599, 1271, 3179, 442, 3396, 218, 2834, 1446, 2067, 1971, 3010, 3320, 332, 251, 354, 842, 1963, 1475, 1196, 2579, 718, 2609, 1106, 742, 1731, 1462, 1315, 3371, 612, 1957, 408, 3390, 2517, 1964, 2467, 857, 1376, 3473, 1483, 0, 736, 3295, 3347, 1502, 2846, 3319, 1073, 247, 423, 1552, 3131, 2426, 2087, 596, 211, 705, 1283, 346, 270, 2184, 2486, 3478, 324, 1159, 149, 3165, 175, 278, 2874, 1670, 657, 1075, 790, 2218, 416, 1398, 217, 3360, 605, 2400, 1251, 1377, 3040, 871, 449, 1067, 818, 2679, 433, 2793, 371, 2864, 2092, 1164, 2172, 2615, 1288, 2541, 3023, 3088, 246, 505, 1402, 554, 1777, 3007, 2682, 3085, 1936, 2876, 1877, 2724, 1006, 3402, 990, 2077, 3268, 485, 1965, 1110, 258, 1779, 2147, 2066, 2650, 3192, 2471, 1869, 932, 2762, 2104, 2358, 1747, 1506, 1807, 296, 2419, 507, 7, 2678, 1565, 2015, 2752, 2210, 3354, 1831, 1602, 1055, 96, 2538, 927, 2238, 2215, 25, 2844, 807, 1044, 2571, 2078, 1822, 67, 1675, 802, 1612, 1840, 1421, 31, 1979, 2526, 610, 1954, 3226, 343, 1452, 2510, 2923, 2857, 2186, 1988, 794, 3039, 1667, 2254, 1487, 576, 2499, 744, 1350, 3013, 528, 176, 2982, 422, 3311, 1558, 1231, 776, 729, 1978, 288, 829, 2314, 1769, 1967, 177, 746, 2753, 1330, 1898, 370, 1613, 598, 3348, 2861, 3109, 2992, 764, 2925, 1873, 141, 1795, 1748, 1780, 654, 471, 1011, 3032, 1104, 2588, 2107, 1094, 195, 532, 3450, 2560, 3350, 1235, 1764, 555, 1808, 2639, 2409, 867, 2843, 631, 1583, 1117, 162, 1789, 3317, 2741, 2144, 1226, 547, 2302]\n",
      "train_index: [889, 1713, 100, 315, 2524, 785, 746, 662, 261, 1316, 1736, 1814, 1313, 179, 1025, 859, 1237, 1987, 480, 655, 2260, 1778, 2275, 819, 2240, 2225, 1578, 1990, 1655, 1911, 755, 247, 2531, 1842, 347, 1877, 998, 2113, 1537, 2438, 765, 1477, 445, 1068, 847, 838, 2779, 1360, 2439, 1973, 1533, 263, 1091, 1572, 353, 208, 803, 705, 1681, 1514, 1423, 57, 807, 2692, 1023, 229, 1488, 2093, 1174, 1779, 1558, 2012, 2011, 69, 1498, 1461, 1036, 2125, 620, 2745, 1048, 2495, 2078, 845, 1594, 2782, 1865, 279, 1560, 678, 2755, 1432, 1835, 596, 1117, 1932, 2525, 440, 508, 76, 2325, 649, 522, 2261, 2394, 2009, 789, 1340, 1161, 1790, 630, 1317, 1985, 482, 1670, 2379, 2757, 438, 952, 1910, 1720, 166, 59, 611, 530, 1475, 49, 1426, 1187, 767, 48, 1729, 2308, 2498, 1001, 632, 1135, 945, 1128, 25, 916, 2106, 1962, 964, 1936, 361, 2637, 1344, 1448, 2162, 1601, 2204, 2442, 2263, 1832, 240, 2007, 582, 2035, 2519, 1213, 1449, 350, 2551, 1102, 2134, 1943, 2151, 1029, 2176, 1381, 657, 1885, 1018, 1146, 2290, 839, 2690, 1133, 961, 538, 1807, 1322, 2201, 2031, 2501, 2477, 809, 1745, 1953, 1336, 123, 1127, 2656, 618, 406, 2714, 1925, 1385, 497, 2192, 342, 2103, 1057, 299, 2488, 2620, 1983, 962, 1849, 1462, 220, 1413, 1302, 449, 1274, 84, 1298, 621, 266, 1942, 290, 1972, 1858, 1285, 416, 472, 940, 2543, 1420, 2296, 2696, 654, 2108, 1866, 1750, 343, 2756, 2775, 1286, 278, 1468, 1407, 1822, 1271, 18, 613, 643, 1536, 1588, 1467, 1508, 1888, 1425, 1033, 2373, 2383, 1506, 1847, 1410, 1883, 680, 1374, 1337, 2262, 1253, 219, 601, 1876, 1472, 303, 52, 1933, 1186, 1587, 912, 1093, 2635, 344, 1869, 1632, 1338, 647, 2448, 1334, 1641, 2232, 1061, 2424, 900, 1988, 192, 801, 174, 2630, 874, 1811, 2203, 351, 668, 932, 128, 340, 7, 1418, 1576, 2254, 307, 812, 637, 383, 2598, 2347, 2581, 2242, 177, 426, 494, 183, 1562, 1255, 184, 2682, 1192, 2257, 2189, 810, 2208, 2211, 1096, 1225, 1058, 489, 2553, 1860, 690, 2515, 978, 1551, 756, 2045, 1900, 1421, 118, 2099, 2512, 281, 2440, 979, 2163, 2600, 2611, 881, 2251, 1395, 1922, 718, 366, 1684, 2230, 555, 2516, 1798, 304, 1599, 203, 1190, 2266, 120, 283, 1647, 575, 1621, 2069, 2596, 2490, 275, 1239, 256, 360, 1034, 1793, 1422, 604, 1606, 1879, 629, 1904, 1920, 2387, 286, 2080, 2052, 300, 1743, 1289, 1378, 423, 2145, 2344, 2399, 162, 65, 2044, 856, 890, 2129, 2510, 742, 2037, 2536, 230, 2016, 2476, 1476, 1185, 1760, 1644, 2633, 1075, 2693, 836, 1437, 374, 2780, 1896, 1812, 1561, 297, 598, 2578, 751, 99, 2291, 1490, 866, 736, 1905, 1777, 1543, 529, 2274, 642, 1507, 636, 2451, 185, 1727, 2686, 937, 2367, 2185, 931, 1706, 1210, 2126, 2193, 511, 1222, 2223, 2269, 1993, 2475, 607, 1177, 1307, 561, 151, 2416, 1939, 731, 1173, 1391, 2535, 23, 1609, 158, 861, 902, 1251, 2092, 444, 2728, 2550, 923, 2248, 311, 548, 2455, 1272, 306, 287, 694, 308, 2148, 749, 1672, 908, 1902, 2206, 1124, 2316, 970, 13, 873, 808, 514, 2238, 409, 1196, 2098, 1780, 1948, 1457, 382, 1368, 2021, 1874, 552, 222, 2414, 8, 1288, 316, 427, 1991, 105, 2382, 1878, 121, 1934, 2662, 2564, 1200, 115, 1228, 2028, 1940, 2601, 2737, 1389, 1554, 672, 1450, 591, 1226, 888, 2768, 1273, 213, 1740, 1675, 800, 2303, 1326, 2116, 2472, 2518, 273, 71, 2032, 1087, 394, 583, 1709, 1717, 500, 1746, 2063, 2765, 2546, 2622, 1505, 205, 733, 526, 1206, 1978, 377, 2020, 2607, 1261, 386, 198, 1050, 590, 519, 1999, 429, 1083, 207, 491, 2704, 792, 1749, 1521, 1397, 83, 2195, 2066, 20, 1208, 1357, 2259, 2500, 2376, 973, 2593, 883, 1098, 2175, 2111, 886, 2648, 2542, 1784, 2158, 1797, 2459, 458, 2005, 78, 2706, 1056, 906, 1362, 2071, 2503, 380, 1882, 2216, 715, 1659, 686, 2381, 305, 209, 1295, 2738, 2761, 1339, 885, 1419, 2212, 2366, 2340, 191, 1510, 483, 915, 277, 988, 1270, 1711, 571, 993, 585, 1735, 2777, 1967, 674, 1465, 855, 447, 1241, 1446, 2097, 2482, 2166, 2479, 294, 588, 1244, 2187, 1247, 2091, 425, 534, 1054, 1616, 2326, 1406, 132, 1592, 1582, 1854, 2468, 1756, 2554, 556, 1771, 1399, 2486, 570, 352, 91, 1914, 2353, 1170, 1299, 1281, 1264, 697, 2328, 348, 1359, 2173, 2003, 1137, 1569, 1269, 2638, 2624, 2312, 453, 913, 1199, 1618, 37, 974, 2575, 983, 1563, 1701, 39, 1544, 169, 737, 1350, 2506, 2636, 634, 2769, 1290, 579, 1908, 1427, 892, 2748, 780, 159, 2458, 1436, 2164, 61, 849, 1921, 2685, 1043, 2318, 1677, 2627, 1070, 53, 1924, 505, 752, 2067, 1769, 1730, 2256, 2781, 1266, 2289, 771, 2606, 1518, 2492, 1085, 1486, 1325, 1894, 1739, 1455, 1456, 2460, 1163, 963, 2304, 398, 584, 948, 704, 431, 292, 145, 2645, 2457, 1165, 2, 576, 2131, 339, 1380, 925, 265, 376, 1615, 271, 696, 2014, 244, 1898, 476, 2004, 2087, 2029, 1452, 2742, 1947, 1620, 250, 413, 545, 1193, 738, 310, 893, 1131, 2699, 15, 1691, 721, 2445, 981, 1624, 2350, 2247, 363, 1731, 182, 985, 1726, 1194, 1559, 1451, 1121, 187, 619, 1871, 1175, 2752, 2040, 614, 1862, 994, 934, 371, 1667, 1971, 593, 726, 826, 687, 2539, 136, 628, 334, 1837, 2733, 1109, 1501, 2505, 710, 1039, 2165, 1992, 1795, 2562, 2341, 1112, 465, 1223, 2406, 2753, 2174, 1104, 1725, 338, 1803, 1662, 2684, 1591, 1950, 2708, 816, 1067, 274, 2083, 865, 101, 1929, 107, 1614, 2772, 2446, 2590, 1830, 1702, 2089, 2168, 2310, 1861, 712, 72, 2377, 2678, 481, 2059, 1766, 2754, 1171, 2276, 1829, 2057, 1783, 405, 757, 2652, 2018, 1628, 97, 1333, 2215, 743, 806, 2574, 1642, 2321, 2358, 1742, 148, 1532, 2359, 2529, 1786, 692, 818, 1386, 2114, 1791, 2281, 701, 1627, 2466, 921, 1424, 905, 454, 1844, 2773, 1530, 167, 730, 1963, 1310, 26, 1674, 2320, 1747, 997, 1176, 1040, 2019, 1566, 2194, 1379, 41, 1550, 2024, 2421, 1710, 1567, 609, 1132, 28, 441, 2771, 1965, 493, 1258, 1301, 2549, 1382, 1304, 236, 551, 520, 2058, 1232, 1138, 720, 735, 2463, 58, 631, 1181, 2043, 592, 1303, 1387, 2430, 2443, 1600, 939, 2592, 1249, 904, 1279, 2228, 2672, 2509, 2213, 1770, 1142, 2348, 706, 2453, 1022, 745, 1546, 2183, 2051, 513, 81, 2471, 1308, 2713, 914, 1404, 2565, 1108, 150, 774, 329, 6, 943, 2634, 734, 909, 468, 181, 164, 1833, 451, 490, 1738, 1143, 1311, 2371, 1111, 2653, 1441, 1637, 1846, 1074, 2679, 1880, 2172, 2533, 2133, 327, 442, 1979, 1718, 832, 2522, 2521, 1414, 1201, 1499, 2298, 86, 1403, 2717, 1481, 777, 1857, 54, 2188, 1287, 2402, 2236, 1245, 113, 2610, 987, 501, 2570, 1009, 1198, 1394, 589, 768, 17, 1042, 1447, 2286, 285, 2569, 428, 1328, 1716, 669, 31, 1341, 722, 951, 2124, 214, 989, 1164, 326, 560, 1383, 1086, 2159, 341, 1653, 147, 2167, 2628, 1107, 848, 966, 1072, 2287, 155, 665, 1995, 1002, 2778, 2337, 176, 2464, 1524, 2621, 1203, 2030, 2614, 1732, 2079, 2130, 875, 2502, 938, 2504, 1552, 346, 2329, 2758, 1765, 385, 1966, 2330, 2034, 243, 2222, 936, 1689, 2396, 312, 1459, 2716, 2284, 1332, 968, 1454, 660, 935, 370, 566, 1024, 824, 982, 2252, 1825, 1645, 2398, 2084, 1909, 813, 1430, 778, 1320, 1889, 2666, 1052, 2149, 1013, 1401, 2086, 2315, 2407, 126, 887, 2280, 1347, 1007, 355, 1003, 1483, 682, 827, 1767, 1831, 1850, 984, 1233, 2369, 2563, 140, 217, 2288, 362, 2474, 345, 1805, 22, 711, 1157, 2142, 871, 1815, 10, 2241, 2107, 2641, 793, 1315, 754, 626, 94, 1818, 129, 1708, 843, 323, 2411, 2119, 1284, 1671, 2698, 14, 2096, 947, 215, 2022, 2221, 1182, 805, 2181, 864, 1658, 516, 1545, 597, 195, 1277, 288, 950, 2619, 2375, 2654, 1120, 1956, 549, 156, 419, 922, 2039, 477, 1997, 1692, 2053, 2659, 1523, 1471, 553, 1202, 390, 578, 1669, 2579, 2042, 2526, 1639, 820, 2664, 1006, 1248, 2033, 664, 2470, 142, 2428, 1046, 421, 1660, 1714, 799, 2210, 615, 1318, 295, 401, 88, 2283, 1772, 2740, 131, 1957, 1945, 77, 1402, 2292, 2417, 1188, 724, 1314, 2179, 1851, 269, 2127, 2309, 2171, 2677, 1372, 1855, 2418, 917, 2465, 1319, 2388, 2669, 587, 1980, 2774, 375, 333, 2132, 2617, 1604, 1840, 1118, 1960, 1442, 2345, 910, 776, 2452, 328, 2342, 1342, 320, 1619, 1665, 2271, 12, 897, 171, 1673, 260, 2180, 689, 901, 2650, 258, 844, 1891, 2566, 138, 1917, 1828, 1573, 319, 653, 2426, 0, 558, 139, 112, 531, 224, 723, 2727, 2586, 2386, 1433, 325, 2157, 1323, 2246, 1458, 2073, 2508, 714, 846, 193, 1464, 573, 946, 503, 1629, 2144, 117, 645, 1870, 3, 74, 1755, 467, 1345, 5, 1246, 919, 547, 228, 1063, 542, 242, 898, 2112, 1160, 822, 2313, 2335, 349, 2395, 2478, 842, 1373, 2354, 2128, 1526, 1605, 2589, 276, 661, 2155, 2739, 2626, 1764, 1989, 2178, 314, 1975, 1466, 1761, 1343, 1305, 691, 1541, 612, 424, 1577, 594, 2759, 1392, 2561, 2494, 622, 33, 1493, 2023, 227, 110, 1169, 108, 232, 66, 1417, 2514, 24, 1235, 2419, 144, 536, 811, 125, 1497, 85, 1254, 2307, 1976, 2285, 2154, 2060, 484, 1654, 2008, 2483, 2102, 2231, 1774, 918, 1646, 1026, 1030, 882, 1335, 2697, 364, 27, 473, 525, 725, 55, 638, 302, 2423, 1276, 2447, 624, 2055, 1280, 1049, 336, 47, 106, 1079, 1144, 1836, 1753, 1101, 1968, 523, 1428, 2422, 603, 1489, 2177, 60, 2555, 671, 92, 666, 1887, 280, 1719, 359, 899, 1935, 1994, 760, 2436, 42, 204, 1473, 2100, 68, 1875, 2480, 137, 796, 758, 541, 1019, 2121, 165, 248, 1331, 2017, 446, 1358, 1329, 2702, 434, 2122, 577, 2270, 2547, 1848, 762, 388, 235, 2420, 1984, 267, 770, 437, 673, 249, 1574, 2081, 2595, 2384, 2545, 448, 1469, 716, 2299, 2429, 2048, 1901, 1168, 82, 2249, 1460, 1893, 1668, 2000, 475, 517, 404, 384, 2258, 2152, 1633, 2657, 688, 1240, 62, 852, 2557, 1262, 223, 373, 1824, 2356, 2665, 2413, 2491, 9, 1944, 1838, 708, 255, 1617, 389, 1581, 924, 2036, 2229, 1224, 1946, 739, 40, 868, 539, 2332, 1371, 1145, 1031, 372, 667, 357, 967, 1353, 1035, 470, 1396, 2741, 1549, 1256, 2094, 1555, 2405, 1250, 1699, 1252, 2585, 79, 1172, 814, 1852, 2220, 2441, 2105, 133, 1140, 933, 928, 823, 1531, 1214, 504, 2751, 1996, 1958, 1384, 894, 828, 2711, 1683, 2390, 2374, 2334, 652, 172, 38, 2658, 713, 90, 2743, 1205, 2401, 1827, 1155, 1324, 216, 510, 1680, 499, 2302, 1856, 1388, 1519, 2527, 2076, 1540, 1217, 1416, 2217, 2339, 1676, 1197, 1542, 953, 750, 234, 732, 2776, 1156, 2389, 1209, 1065, 2156, 1062, 1167, 1492, 2297, 884, 143, 2732, 850, 717, 2642, 264, 75, 395, 2364, 2224, 2603, 396, 1365, 2322, 1903, 595, 2109, 1296, 1821, 872, 2703, 1548, 36, 2346, 2143, 2372, 977, 1571, 1122, 2239, 1892, 675, 728, 412, 1516, 1649, 825, 616, 114, 417, 1625, 2729, 89, 2082, 753, 1630, 11, 284, 1066, 1191, 1440, 2605, 2688, 2517, 1568, 2548, 1687, 851, 2207, 895, 496, 2305, 1782, 335, 515, 1348, 1015, 2735, 1781, 1785, 2070, 356, 2651, 1312, 1635, 550, 2265, 1227, 2487, 957, 1773, 2141, 1008, 1092, 2481, 996, 1679, 153, 2587, 580, 1470, 703, 1212, 896, 487, 559, 1141, 1119, 512, 633, 2400, 1664, 1012, 1931, 1150, 2609, 116, 740, 2695, 1974, 1081, 1810, 830, 119, 656, 2538, 2499, 2722, 635, 369, 268, 46, 991, 876, 773, 1219, 1126, 4, 1595, 2311, 1077, 837, 681, 1715, 2352, 2361, 2197, 1148, 574, 50, 853, 1982, 1823, 833, 1115, 1166, 1906, 971, 399, 2368, 19, 684, 35, 2631, 524, 920, 1841, 1890, 537, 1705, 2721, 245, 784, 1409, 154, 1686, 625, 2670, 1346, 569, 1758, 1816, 2065, 127, 2763, 2006, 980, 2760, 190, 1139, 992, 699, 606, 180, 301, 2580, 1060, 2623, 2671, 1069, 2205, 1243, 16, 2766, 546, 658, 2573, 2541, 1504, 959, 2331, 797, 225, 2074, 1799, 2485, 2749, 2277, 2085, 1693, 1045, 469, 954, 2762, 639, 2583, 1218, 1158, 1415, 2730, 1527, 2712, 1300, 2663, 1603, 160, 1704, 956, 2707, 877, 1919, 1884, 2200, 98, 2629, 253, 103, 1445, 1443, 1648, 586, 627, 1696, 2101, 2199, 403, 1154, 2255, 1682, 2056, 2644, 452, 2301, 1327, 2049, 880, 2709, 623, 2393, 2577, 1787, 262, 1634, 1438, 2198, 1969, 2520, 1802, 640, 2385, 2632, 1959, 488, 1843, 1734, 146, 1534, 1986, 1369, 2450, 2726, 659, 1794, 2511, 186, 1147, 608, 2191, 1930, 197, 2327, 293, 766, 400, 122, 202, 835, 2294, 1439, 1733, 1695, 1707, 2705, 1153, 1661, 2432, 1759, 854, 1589, 2267, 641, 1, 2160, 698, 1636, 1291, 2655, 2489, 1520, 1853, 1136, 2691, 1631, 1998, 663, 683, 1257, 317, 648, 1434, 1479, 709, 1282, 1768, 972, 1306, 1400, 1408, 2237, 1496, 2038, 1529, 1663, 1354, 2715, 2750, 1643, 1051, 795, 1597, 2604, 2625, 1819, 1722, 1183, 2572, 1059, 540, 1678, 2681, 2532, 1757, 1923, 95, 1020, 563, 863, 1484, 1881, 1038, 206, 392, 870, 397, 2675, 1028, 1485, 2660, 960, 1570, 1895, 2146, 804, 1751, 2454, 502, 779, 1863, 200, 2182, 2088, 2278, 1796, 378, 2336, 418, 1698, 391, 2640, 1522, 1162, 1495, 2027, 1152, 1367, 2568, 64, 2062, 1180, 492, 379, 2235, 763, 2264, 791, 1076, 878, 337, 2343, 1016, 1275, 455, 815, 2317, 995, 1981, 201, 161, 1579, 2449, 702, 1500, 1129, 2613, 1021, 1585, 1955, 2253, 34, 775, 2556, 1478, 1390, 2139, 1363, 2612, 2041, 241, 2061, 2363, 600, 2435, 1297, 2214, 2068, 646, 1528, 1267, 1899, 562, 2734, 189, 1806, 975, 2747, 2047, 2558, 1082, 474, 747, 2300, 21, 459, 1184, 2324, 955, 1215, 2433, 1515, 2391, 769, 1685, 130, 2135, 1482, 330, 1238, 466, 2169, 1638, 1095, 1130, 1294, 860]\n",
      "val_index: [2457, 1114, 1298, 1442, 1665, 51, 322, 2653, 2164, 676, 1192, 1864, 2815, 2742, 134, 2004, 1623, 3107, 1320, 1842, 2037, 693, 184, 2658, 192, 1904, 3271, 450, 1146, 621, 2832, 538, 2266, 1537, 2801, 26, 999, 1718, 179, 32, 1468, 1490, 921, 2804, 2383, 2932, 1937, 893, 70, 1068, 2248, 691, 443, 2835, 1584, 298, 1429, 1878, 1736, 678, 1786, 670, 969, 183, 978, 1553, 2381, 3442, 257, 3269, 1234, 1258, 2910, 3019, 2543, 1615, 578, 2603, 1666, 3330, 557, 139, 1966, 2473, 1432, 2115, 295, 1738, 1784, 196, 2484, 2967, 1556, 925, 3229, 942, 2739, 3204, 52, 3374, 1037, 3167, 1091, 495, 3124, 170, 1662, 3310, 478, 309, 2836, 402, 76, 2740, 965, 2059, 17, 3255, 144, 3055, 602, 551, 2023, 2943, 3361, 2189, 3315, 1173, 2717, 3394, 1765, 2957, 1034, 1501, 194, 1871, 1228, 1569, 2154, 3148, 1897, 1832, 1746, 1818, 1600, 30, 879, 1626, 2367, 2643, 985, 869, 2551, 203, 501, 2534, 897, 1313, 3206, 1397, 3344, 2891, 2587, 1674, 1374, 33, 1023, 948, 665, 2830, 3111, 1018, 949, 3197, 3358, 1805, 229, 411, 2475, 1151, 2241, 567, 2313, 2775, 289, 432, 2656, 3474, 410, 1798, 414, 3153, 937, 712, 2599, 3264, 1702, 3329, 2149, 1309, 864, 415, 881, 1742, 952, 811, 279, 781, 479, 2044, 2226, 2323, 80, 1041, 2980, 93, 3240, 1395, 1749, 511, 3276, 463, 1253, 2086, 3362, 2029, 1457, 1593, 1222, 358, 2862, 1729, 1721, 3430, 839, 2272, 3458, 1809, 2339, 798, 1052, 1825, 1860, 506, 166, 1123, 2533, 1211, 3220, 3469, 787, 1467, 430, 874, 2123, 238, 315, 2930, 2732, 1200, 752, 174, 1419, 1420, 1406, 3290, 321, 3267, 2394, 102, 299, 239, 2018, 1071, 325, 761, 2428, 2208, 2803, 44, 1336, 964, 2380, 1351, 1161, 457, 291, 3245, 58, 679, 3472, 387, 3423, 759, 912, 1345, 3288, 2090, 2249, 1340, 1691, 314, 816, 461, 2500, 188, 1322, 2202, 1213, 1427, 393, 240, 2552, 3014, 3243, 1105, 2707, 2757, 1370, 2168, 2271, 152, 2443, 2364, 3401, 573, 256, 1061, 1703, 2345, 1756, 544, 3435, 1922, 2828, 178, 1833, 3035, 439, 3096, 1455, 3462, 2602, 2247, 594, 2461, 14, 486, 173, 976, 331, 1580, 1803, 2020, 1080, 1960, 3332, 2654, 929, 1352, 3052, 1497, 2315, 680, 1586, 527, 785, 63, 266, 1399, 564, 157, 3121, 2093, 2954, 1391, 254, 2369, 1618, 2436, 2407, 124, 1261, 1745, 263, 2308, 3291, 3221, 3173, 1187, 2316, 3287, 2756, 438, 88, 2441, 840, 1624, 2124, 1642, 1206, 1532, 2418, 3000, 3140, 1444, 120, 3359, 2516, 2570, 2764, 2328, 1767, 642, 1074, 283, 1883, 274, 721, 109, 1113, 940, 214, 2456, 3048, 1292, 1232, 3304, 1032, 368, 1812, 1517, 862, 2259, 2513, 1116, 1392, 1582, 2341, 2728, 45, 599, 1271, 3179, 442, 3396, 218, 2834, 1446, 2067, 1971, 3010, 3320, 332, 251, 354, 842, 1963, 1475, 1196, 2579, 718, 2609, 1106, 742, 1731, 1462, 1315, 3371, 612, 1957, 408, 3390, 2517, 1964, 2467, 857, 1376, 3473, 1483, 0, 736, 3295, 3347, 1502, 2846, 3319, 1073, 247, 423, 1552, 3131, 2426, 2087, 596, 211, 705, 1283, 346, 270, 2184, 2486, 3478, 324, 1159, 149, 3165, 175, 278, 2874, 1670, 657, 1075, 790, 2218, 416, 1398, 217, 3360, 605, 2400, 1251, 1377, 3040, 871, 449, 1067, 818, 2679, 433, 2793, 371, 2864, 2092, 1164, 2172, 2615, 1288, 2541, 3023, 3088, 246, 505, 1402, 554, 1777, 3007, 2682, 3085, 1936, 2876, 1877, 2724, 1006, 3402, 990, 2077, 3268, 485, 1965, 1110, 258, 1779, 2147, 2066, 2650, 3192, 2471, 1869, 932, 2762, 2104, 2358, 1747, 1506, 1807, 296, 2419, 507, 7, 2678, 1565, 2015, 2752, 2210, 3354, 1831, 1602, 1055, 96, 2538, 927, 2238, 2215, 25, 2844, 807, 1044, 2571, 2078, 1822, 67, 1675, 802, 1612, 1840, 1421, 31, 1979, 2526, 610, 1954, 3226, 343, 1452, 2510, 2923, 2857, 2186, 1988, 794, 3039, 1667, 2254, 1487, 576, 2499, 744, 1350, 3013, 528, 176, 2982, 422, 3311, 1558, 1231, 776, 729, 1978, 288, 829, 2314, 1769, 1967, 177, 746, 2753, 1330, 1898, 370, 1613, 598, 3348, 2861, 3109, 2992, 764, 2925, 1873, 141, 1795, 1748, 1780, 654, 471, 1011, 3032, 1104, 2588, 2107, 1094, 195, 532, 3450, 2560, 3350, 1235, 1764, 555, 1808, 2639, 2409, 867, 2843, 631, 1583, 1117, 162, 1789, 3317, 2741, 2144, 1226, 547, 2302]\n",
      "x_train: (2227, 90, 90, 15)\n",
      "y_train: (2227, 1)\n",
      "x_val shape: (557, 90, 90, 15)\n",
      "y_val shape: (557, 1)\n",
      "x_test shape: (696, 90, 90, 15)\n",
      "y_test shape: (696, 1)\n"
     ]
    }
   ],
   "source": [
    "# Initialize empty lists to store data\n",
    "x_train_list = []\n",
    "y_train_list = []\n",
    "agb_values_list = []\n",
    "\n",
    "# Loop through each patch with tqdm for progress bars\n",
    "# Loop through each patch with tqdm for progress bars\n",
    "for i in tqdm(range(desired_num_patches), desc='Loading Patches'):\n",
    "    # Get file names for the current index\n",
    "    optic_file = optic_image_files[i]\n",
    "    sar_file = sar_image_files[i]\n",
    "    agb_file = agb_map_files[i]\n",
    "    # Load images\n",
    "    optic_image = io.imread(os.path.join(optic_images_dir, optic_file))\n",
    "    sar_image = io.imread(os.path.join(sar_images_dir, sar_file))\n",
    "    agb_map = io.imread(os.path.join(agb_maps_dir, agb_file))\n",
    "\n",
    "    # Save AGB values as a NumPy array\n",
    "    agb_values_list.append(agb_map.flatten())\n",
    "\n",
    "    # Ensure optic image has 13 bands\n",
    "    if optic_image.shape[-1] == 13:\n",
    "        x_train = optic_image\n",
    "    else:\n",
    "        raise ValueError(\"Optic image should have 13 bands. Found: {}\".format(optic_image.shape[-1]))\n",
    "\n",
    "    # Check if SAR image has 2 bands\n",
    "    if len(sar_image.shape) == 2:\n",
    "        # Expand dimensions to make it 3D\n",
    "        sar_image = np.stack([sar_image, sar_image], axis=-1)\n",
    "\n",
    "    # Normalize the data using min-max scaling\n",
    "    optic_image_normalized = (optic_image - np.min(optic_image)) / (np.max(optic_image) - np.min(optic_image))\n",
    "    sar_image_normalized = (sar_image - np.min(sar_image)) / (np.max(sar_image) - np.min(sar_image))\n",
    "\n",
    "    # Concatenate optic and SAR images\n",
    "    x_train = np.concatenate([optic_image_normalized, sar_image_normalized], axis=-1)\n",
    "\n",
    "    x_train_list.append(x_train)\n",
    "\n",
    "    # Break the loop when desired_num_patches is reached\n",
    "    if len(x_train_list) >= desired_num_patches:\n",
    "        break\n",
    "\n",
    "# Combine all patches into single arrays\n",
    "x_data = np.stack(x_train_list)\n",
    "agb_values = np.concatenate(agb_values_list)\n",
    "\n",
    "# Normalize AGB values using min-max scaling\n",
    "min_agb = np.min(agb_values)\n",
    "max_agb = np.max(agb_values)\n",
    "agb_scaled = (agb_values - min_agb) / (max_agb - min_agb)\n",
    "\n",
    "# Reshape scaled AGB values for compatibility with the model\n",
    "agb_normalized = np.expand_dims(agb_scaled, axis=-1)\n",
    "\n",
    "# Split the data into training, validation, and test sets\n",
    "\n",
    "x_train1, x_test, y_train1, y_test, _ , test_index = train_test_split(x_data, agb_normalized,range(len(x_data)), test_size=0.20, random_state=42)\n",
    "x_train, x_val, y_train, y_val, train_index, val_index = train_test_split(x_train1, y_train1, range(len(x_train1)), test_size=0.20, random_state=42)\n",
    "\n",
    "print('test_index:', test_index)\n",
    "print('train_index:', train_index)\n",
    "print('val_index:', test_index)\n",
    "print('x_train:', x_train.shape)\n",
    "print('y_train:', y_train.shape)\n",
    "print('x_val shape:', x_val.shape)\n",
    "print('y_val shape:', y_val.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "print('y_test shape:', y_test.shape)\n",
    "input_shape = x_train.shape[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "234600\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(min_agb)\n",
    "print(max_agb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\irannejad\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "(None, 1024)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "## Configure the hyperparameters\n",
    "\"\"\"\n",
    "num_classes = 1\n",
    "learning_rate = 0.00001\n",
    "weight_decay = 0.0001\n",
    "batch_size = 8\n",
    "num_epochs = 150  # For real training, use num_epochs=100. 10 is a test value\n",
    "image_size = 96  # We'll resize input images to this size\n",
    "patch_size = 6  # Size of the patches to be extract from the input images\n",
    "num_patches = (image_size // patch_size) ** 2\n",
    "projection_dim = 64\n",
    "num_heads = 8\n",
    "transformer_units = [\n",
    "    projection_dim * 2,\n",
    "    projection_dim,\n",
    "]  # Size of the transformer layers\n",
    "transformer_layers = 8\n",
    "mlp_head_units = [\n",
    "    2048,\n",
    "    1024,\n",
    "]  # Size of the dense layers of the final classifier\n",
    "\"\"\"\n",
    "## Use data augmentation\n",
    "\"\"\"\n",
    "\n",
    "data_augmentation = keras.Sequential(\n",
    "    [\n",
    "        layers.Normalization(),\n",
    "        layers.Resizing(image_size, image_size),\n",
    "        layers.RandomFlip(\"horizontal\"),\n",
    "        layers.RandomRotation(factor=0.02),\n",
    "        layers.RandomZoom(height_factor=0.2, width_factor=0.2),\n",
    "    ],\n",
    "    name=\"data_augmentation\",\n",
    ")\n",
    "# Compute the mean and the variance of the training data for normalization.\n",
    "data_augmentation.layers[0].adapt(x_train)\n",
    "\"\"\"\n",
    "## Implement multilayer perceptron (MLP)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def mlp(x, hidden_units, dropout_rate):\n",
    "    for units in hidden_units:\n",
    "        x = layers.Dense(units, activation=keras.activations.relu)(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "    return x\n",
    "\"\"\"\n",
    "## Implement patch creation as a layer\n",
    "\"\"\"\n",
    "\n",
    "class Patches(layers.Layer):\n",
    "    def __init__(self, patch_size):\n",
    "        super(Patches, self).__init__()\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "    def call(self, images):\n",
    "        batch_size = tf.shape(images)[0]\n",
    "        patches = tf.image.extract_patches(\n",
    "            images=images,\n",
    "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
    "            strides=[1, self.patch_size, self.patch_size, 1],\n",
    "            rates=[1, 1, 1, 1],\n",
    "            padding='VALID'\n",
    "        )\n",
    "        patches = tf.reshape(\n",
    "            patches,\n",
    "            (batch_size, -1, self.patch_size * self.patch_size * 15)\n",
    "        )\n",
    "        return patches\n",
    "\n",
    "class PatchEncoder(layers.Layer):\n",
    "    def __init__(self, num_patches, projection_dim):\n",
    "        super(PatchEncoder, self).__init__()\n",
    "        self.num_patches = num_patches\n",
    "        self.projection = layers.Dense(units=projection_dim)\n",
    "        self.position_embedding = layers.Embedding(\n",
    "            input_dim=num_patches, output_dim=projection_dim\n",
    "        )\n",
    "\n",
    "    def call(self, patch):\n",
    "        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
    "        positions = tf.expand_dims(positions, axis=0)\n",
    "        projected_patches = self.projection(patch)\n",
    "        encoded = projected_patches + self.position_embedding(positions)\n",
    "        return encoded\n",
    "\n",
    "def mlp(x, hidden_units, dropout_rate):\n",
    "    for units in hidden_units:\n",
    "        x = layers.Dense(units, activation=tf.keras.activations.gelu)(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "    return x\n",
    "\"\"\"\n",
    "## Build the ViT model\n",
    "\n",
    "The ViT model consists of multiple Transformer blocks,\n",
    "which use the `layers.MultiHeadAttention` layer as a self-attention mechanism\n",
    "applied to the sequence of patches. The Transformer blocks produce a\n",
    "`[batch_size, num_patches, projection_dim]` tensor, which is processed via an\n",
    "classifier head with softmax to produce the final class probabilities output.\n",
    "\n",
    "Unlike the technique described in the [paper](https://arxiv.org/abs/2010.11929),\n",
    "which prepends a learnable embedding to the sequence of encoded patches to serve\n",
    "as the image representation, all the outputs of the final Transformer block are\n",
    "reshaped with `layers.Flatten()` and used as the image\n",
    "representation input to the classifier head.\n",
    "Note that the `layers.GlobalAveragePooling1D` layer\n",
    "could also be used instead to aggregate the outputs of the Transformer block,\n",
    "especially when the number of patches and the projection dimensions are large.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def create_vit_classifier():\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "    # Augment data.\n",
    "    augmented = data_augmentation(inputs)\n",
    "    # Create patches.\n",
    "    patches = Patches(patch_size)(augmented)\n",
    "    # Encode patches.\n",
    "    encoded_patches = PatchEncoder(num_patches, projection_dim)(patches)\n",
    "\n",
    "    # Create multiple layers of the Transformer block.\n",
    "    for _ in range(transformer_layers):\n",
    "        # Layer normalization 1.\n",
    "        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "        # Create a multi-head attention layer.\n",
    "        attention_output = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n",
    "        )(x1, x1)\n",
    "        # Skip connection 1.\n",
    "        x2 = layers.Add()([attention_output, encoded_patches])\n",
    "        # Layer normalization 2.\n",
    "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
    "        # MLP.\n",
    "        x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)\n",
    "        # Skip connection 2.\n",
    "        encoded_patches = layers.Add()([x3, x2])\n",
    "\n",
    "    # Create a [batch_size, projection_dim] tensor.\n",
    "    representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "    representation = layers.Flatten()(representation)\n",
    "    representation = layers.Dropout(0.5)(representation)\n",
    "    # Add MLP.\n",
    "    features = mlp(representation, hidden_units=mlp_head_units, dropout_rate=0.5)\n",
    "    print(features.shape)\n",
    "    # Classify outputs.\n",
    "    logits = layers.Dense(num_classes,\n",
    "                    activation='linear',\n",
    "                    )(features)\n",
    "    # Create the Keras model.\n",
    "    model = keras.Model(inputs=inputs, outputs=logits)\n",
    "    return model\n",
    "vit_classifier = create_vit_classifier()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 1024)\n"
     ]
    }
   ],
   "source": [
    "# Create the model using your custom architecture\n",
    "vit_classifier = create_vit_classifier()\n",
    "\n",
    "# Load the weights into the model\n",
    "vit_classifier.load_weights('weights_ViT.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\irannejad\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "vit_classifier.save('complete_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Assuming Patches and PatchEncoder are already defined in your code\n",
    "custom_objects = {\n",
    "    'Patches': Patches,\n",
    "    'PatchEncoder': PatchEncoder\n",
    "}\n",
    "\n",
    "# Now load the model\n",
    "model = load_model('complete_model.h5', custom_objects=custom_objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109/109 [==============================] - 282s 3s/step\n"
     ]
    }
   ],
   "source": [
    "# Predictions on training data\n",
    "y_pred = model.predict(x_data)\n",
    "y_pred = y_pred * ((max_agb - min_agb) + min_agb)  # Denormalize predictions\n",
    "y_denormalized = agb_normalized * ((max_agb - min_agb) + min_agb)  # Denormalize true values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from osgeo import gdal\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "for i in range(3480):\n",
    "\n",
    "  ff = agb_map_files [i].split(\".\")\n",
    "  #print(ff[0])\n",
    "  input_file = gdal.Open(os.path.join(agb_maps_dir, f\"{ff[0]}.tif\"))\n",
    "  #print(input_file.RasterXSize)\n",
    "  # Create the output GeoTIFF file\n",
    "  output_file = gdal.GetDriverByName('GTiff').Create(r\"F:\\VIT\" \n",
    "                                                     + f\"{ff[0]}\" + '.tif', input_file.RasterXSize, \n",
    "                                                     input_file.RasterYSize, 1, gdal.GDT_Float32)\n",
    "\n",
    "  # Set the spatial reference and geotransform of the output file\n",
    "  output_file.SetProjection(input_file.GetProjection())\n",
    "  output_file.SetGeoTransform(input_file.GetGeoTransform())\n",
    "\n",
    "  # Get the output band of the output file\n",
    "  output_band = output_file.GetRasterBand(1)\n",
    "  output_band.WriteArray(y_pred[i].reshape(input_file.RasterXSize,input_file.RasterYSize))\n",
    "\n",
    "  # Compute the output band's statistics\n",
    "  output_band.ComputeStatistics(False)\n",
    "\n",
    "  # Close the input and output files\n",
    "  input_file = None\n",
    "  output_file = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "def copy_images(source_folder, destination_folder, image_list):\n",
    "    if not os.path.exists(destination_folder):\n",
    "        os.makedirs(destination_folder)\n",
    "\n",
    "    for filename in image_list:\n",
    "      filename = str(int(filename) + 1).zfill(5) + \".tif\"\n",
    "      print(filename)\n",
    "      source_path = os.path.join(source_folder, filename)\n",
    "      if os.path.exists(source_path):\n",
    "            destination_path = os.path.join(destination_folder, filename)\n",
    "            shutil.copyfile(source_path, destination_path)\n",
    "            print(f\"Copied {filename} to {destination_folder}\")\n",
    "      else:\n",
    "            print(f\"File {filename} not found in {source_folder}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define source folder containing TIFF images\n",
    "source_folder = \"F:\\output\"\n",
    "\n",
    "# Define destination folder where images will be copied\n",
    "destination_folder_Test = \"F:\\Test_OutPut\"\n",
    "destination_folder_Train = \"F:\\Train_OutPut\"\n",
    "destination_folder_Val = \"F:\\Val_OutPut\"\n",
    "\n",
    "# List of filenames to be copied\n",
    "#image_list = [\"image1.tif\", \"image2.tif\", \"image3.tif\"]  # Add your filenames here\n",
    "\n",
    "copy_images(source_folder, destination_folder_Test, test_index)\n",
    "copy_images(source_folder, destination_folder_Train, train_index)\n",
    "copy_images(source_folder, destination_folder_Val, val_index)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
